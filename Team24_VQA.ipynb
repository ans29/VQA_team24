{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing needed libraries from keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in /home/ansh/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in /home/ansh/anaconda3/lib/python3.6/site-packages (from pydot)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pydot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Multilayer Perceptron\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Activation\n",
    "import tensorflow as tf\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Na = 205\n",
    "Nq = 80\n",
    "question_lstm_hidden_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs\n",
    "### We need to get the following embeddings as input to the model - \n",
    "(1) Image Embedding \n",
    "\n",
    "(2) Answer Embedding\n",
    "\n",
    "(3) Question Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Image Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the image embedding by using VGG16 Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 14, 512)\n"
     ]
    }
   ],
   "source": [
    "import getEm\n",
    "all_img_emb = getEm.get_train_img_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Answer Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Imports for Getting Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /home/ansh/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/ansh/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/ansh/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /home/ansh/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ansh/anaconda3/lib/python3.6/site-packages (from gensim)\n",
      "Requirement already satisfied: requests in /home/ansh/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto>=2.32 in /home/ansh/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in /home/ansh/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in /home/ansh/anaconda3/lib/python3.6/site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ansh/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /home/ansh/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /home/ansh/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ansh/anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ansh/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.11.0,>=1.10.4 in /home/ansh/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /home/ansh/anaconda3/lib/python3.6/site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in /home/ansh/anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<2.7.0,>=2.1 in /home/ansh/anaconda3/lib/python3.6/site-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim\n",
    "from vqaTools.vqa import VQA\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import sys\n",
    "import json\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import preprocess as pre\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Initializing and loading annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading VQA annotations and questions into memory...\n",
      "0:01:54.027877\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "dataDir = '../../'\n",
    "versionType = 'v2_'  # this should be '' when using VQA v2.0 dataset\n",
    "taskType = 'OpenEnded'  # 'OpenEnded' only for v2.0. 'OpenEnded' or 'MultipleChoice' for v1.0\n",
    "dataType = 'mscoco'  # 'mscoco' only for v1.0. 'mscoco' for real and 'abstract_v002' for abstract for v1.0.\n",
    "dataSubType = 'train2014'\t\n",
    "annFile = '%s/%s%s_%s_annotations.json' % (dataDir, versionType, dataType, dataSubType)\n",
    "quesFile = '%s/%s%s_%s_%s_questions.json' % (dataDir, versionType, taskType, dataType, dataSubType)\n",
    "imgDir = '%s/Images/%s/%s/' % (dataDir, dataType, dataSubType)\n",
    "\n",
    "# initialize VQA api for QA annotations\n",
    "vqa = VQA(annFile, quesFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Reduced Annotation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all the annotations Ids\n",
    "annIds = vqa.getImgIds()\n",
    "\n",
    "# Select only first 1000 annotations\n",
    "annotations = annIds[:1001]\n",
    "\n",
    "# Get the list of all unique Image Ids from these 1000 annotations\n",
    "imageIds = []\n",
    "for ann in annotations:\n",
    "    imageIds.append(ann['image_id'])\n",
    "imageIds = set(imageIds)\n",
    "\n",
    "# Select only 100 images from this unique list\n",
    "imageDataSet = list(imageIds)[:101]\n",
    "\n",
    "# Get all the questions related to these 100 images\n",
    "questions = vqa.getQuesIds(imageDataSet)\n",
    "\n",
    "# Making a map of the {'imageID':([List of questions], annotation)}\n",
    "qs = []\n",
    "for ann in annotations:\n",
    "    if ann['question_id'] in questions:\n",
    "        qs.append({'image_id': ann['image_id'], 'question': vqa.getQuestions(ann), 'answer': ann['multiple_choice_answer']})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Answer Embedding from reduced annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n",
      "Final Emb: 205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "# Making a list which will contain all the answers with confidence of yes/maybe\n",
    "answer_set = []\n",
    "for data_entry in qs:\n",
    "    answer_set.append(data_entry['answer'])\n",
    "\n",
    "answer_set = set(answer_set)\n",
    "\n",
    "ans_embedding = []\n",
    "for ans in answer_set:\n",
    "    token = ans.split(\" \")\n",
    "    ans_embedding.append(token)\n",
    "\n",
    "print(len(ans_embedding))\n",
    "\n",
    "embedding = Word2Vec(sentences = ans_embedding, size=512, window=1, min_count = 1, workers=4, sg=0)\n",
    "words = list(embedding.wv.vocab)\n",
    "final_ans_embedding=[]\n",
    "for ans in answer_set:\n",
    "    token = ans.split(\" \")\n",
    "    if len(token) > 1:\n",
    "        index = math.ceil(len(token)/2)\n",
    "        ans = token[index]\n",
    "    final_ans_embedding.append({ans:embedding[ans]})\n",
    "print(\"Final Emb:\", len(final_ans_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Question Embedding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get Question List from the Reduced Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in vocab would be 693\n"
     ]
    }
   ],
   "source": [
    "import vqaLib as lib\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "\n",
    "question_set = []\n",
    "for data_entry in qs:\n",
    "    question_set.append(data_entry['question'])\n",
    "# print(question_set)\n",
    "\n",
    "dataset_train = lib.prepro_question(question_set)\n",
    "#print(dataset_train)\n",
    "dataset_train, vocab = lib.build_vocab_question(dataset_train)\n",
    "itow = {i+1:w for i,w in enumerate(vocab)} # a 1-indexed vocab translation table\n",
    "wtoi = {w:i+1 for i,w in enumerate(vocab)} # inverse table\n",
    "ques_train, ques_length_train, question_id_train = lib.encode_question(dataset_train, wtoi)\n",
    "\n",
    "question_list = []\n",
    "\n",
    "for entry in dataset_train:\n",
    "    question_list.append(entry['final_question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Making the Question Embedding Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "embed = Word2Vec(sentences = question_list, size=512, window=3, min_count = 1, workers=4, sg=0)\n",
    "words = list(embedding.wv.vocab)\n",
    "\n",
    "question_embedding_map = {}\n",
    "question_embedding_list = []\n",
    "#print(len(question_embedding_list[0]))\n",
    "for entry in vocab:\n",
    "    if entry!='?' and entry!='UNK':\n",
    "        question_embedding_map[entry] = embed[entry]\n",
    "\n",
    "print(\"++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "print(len(question_embedding_map))\n",
    "\n",
    "question_embedding_keys = list(question_embedding_map.keys())\n",
    "# print(question_embedding_keys)\n",
    "# print(question_embedding_keys[0])\n",
    "# print(question_embedding_map[question_embedding_keys[0]])\n",
    "\n",
    "question_embedding_list = question_embedding_map.values()\n",
    "question_embedding_list = (list(question_embedding_list))\n",
    "#print(len(question_embedding_list[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting question embeddings for each word in the question and storing it separately in list: [ [que_word1] [que_word2] ... [que_wordn], [ ][ ],  ]. Also image-id corresponding to each question is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "548\n",
      "548\n",
      "548\n",
      "548\n"
     ]
    }
   ],
   "source": [
    "list_ques_embedding_wordWise = []\n",
    "list_imageId_mapped_to_ques = []\n",
    "list_ans_mapped_to_ques = []\n",
    "list_questions = []\n",
    "\n",
    "for data_entry in qs:\n",
    "    question = data_entry['question'][0]\n",
    "    image_id = data_entry['image_id']\n",
    "    \n",
    "    list_questions.append(question)\n",
    "    list_ans_mapped_to_ques.append(data_entry['answer'])\n",
    "    list_imageId_mapped_to_ques.append(image_id)\n",
    "    \n",
    "    question_embedding_wordWise = []\n",
    "    for word in question.split(' '):\n",
    "        \n",
    "        word = lib.tokenize(word)\n",
    "        \n",
    "        if(len(word) > 1 and len(word[-1]) <= 1):\n",
    "            word = word[0]\n",
    "           \n",
    "        word = ''.join(word)\n",
    "        \n",
    "        word = word.lower()\n",
    "        if(word[-1] in ('?', ',')):\n",
    "            continue\n",
    "           \n",
    "        question_embedding_wordWise.append(question_embedding_map[word]) #Here\n",
    "    list_ques_embedding_wordWise.append(question_embedding_wordWise)\n",
    "\n",
    "print(len(list_questions))\n",
    "print(len(list_ques_embedding_wordWise))\n",
    "print(len(list_imageId_mapped_to_ques))\n",
    "print(len(list_ans_mapped_to_ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/ansh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, return_sequences=True, input_shape=(80, 512))`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 80, 256)           787456    \n",
      "=================================================================\n",
      "Total params: 787,456\n",
      "Trainable params: 787,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ansh/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  app.launch_new_instance()\n",
      "/home/ansh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, return_sequences=True, input_shape=(80, 512))`\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 80, 256)           65792     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 80, 256)           525312    \n",
      "=================================================================\n",
      "Total params: 591,104\n",
      "Trainable params: 591,104\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 80, 512)           0         \n",
      "=================================================================\n",
      "Total params: 1,378,560\n",
      "Trainable params: 1,378,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansh/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers import Merge\n",
    "\n",
    "# create the model\n",
    "embedding_vector_length = len(question_embedding_list)\n",
    "question_tensor = tf.convert_to_tensor(np.array(question_embedding_list), dtype=tf.float32)\n",
    "subLstmModel1 = Sequential()\n",
    "subLstmModel1.add(LSTM(256, input_length=Nq, input_dim=len(question_embedding_list[0]), \n",
    "                      return_sequences=True))\n",
    "subLstmModel1.summary()\n",
    "\n",
    "subLstmModel2 = Sequential()\n",
    "subLstmModel2.add(Conv1D(256, 1, activation='relu', input_shape=(Nq, 256)))\n",
    "subLstmModel2.add(LSTM(256, input_length=Nq, input_dim=len(question_embedding_list[0]), \n",
    "                      return_sequences=True))                 \n",
    "subLstmModel2.summary()\n",
    "\n",
    "questionModel = Sequential()\n",
    "questionModel.add(Merge([subLstmModel1, subLstmModel2], mode='concat'))\n",
    "questionModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to calculate  - \n",
    "(1) Unary Potentials\n",
    "\n",
    "(2) Pairwise Potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Unary Potentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question Unary Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 80, 512)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 80, 512)           262656    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 80, 1)             513       \n",
      "=================================================================\n",
      "Total params: 263,169\n",
      "Trainable params: 263,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ques = Input(shape=(Nq,512))\n",
    "conv1 = Conv1D(512, kernel_size=1, activation='tanh')(ques)\n",
    "Qoutput = Conv1D(1, kernel_size=1)(conv1)\n",
    "thetaQ = Model(inputs=ques, outputs=Qoutput)\n",
    "# summarize layers\n",
    "print(thetaQ.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer Unary Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 205, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 205, 512)          262656    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 205, 1)            513       \n",
      "=================================================================\n",
      "Total params: 263,169\n",
      "Trainable params: 263,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "ans = Input(shape=(Na,512))\n",
    "conv2 = Conv1D(512, kernel_size=1, activation='tanh')(ans)\n",
    "Aoutput = Conv1D(1, kernel_size=1)(conv2)\n",
    "thetaA = Model(inputs=ans, outputs=Aoutput)\n",
    "# summarize layers\n",
    "print(thetaA.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Image Unary Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 196, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 196, 512)          262656    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 196, 1)            513       \n",
      "=================================================================\n",
      "Total params: 263,169\n",
      "Trainable params: 263,169\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "img = Input(shape=(196,512))\n",
    "conv3 = Conv1D(512, kernel_size=1, activation='tanh')(img)\n",
    "Voutput = Conv1D(1, kernel_size=1)(conv3)\n",
    "thetaV = Model(inputs=img, outputs=Voutput)\n",
    "# summarize layers\n",
    "print(thetaV.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Pairwise Potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Lambda\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Image - Question Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 15, 1)\n",
      "(1, 196, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "img = Input(shape=(196,512))\n",
    "convImg = Conv1D(512, kernel_size=1, activation=None)(img)\n",
    "\n",
    "ques = Input(shape=(15,512))\n",
    "convQues = Conv1D(512, kernel_size=1, activation=None)(ques)\n",
    "batch_size = 1\n",
    "\n",
    "thetaVQ = K.dot(convImg[batch_size,:,:], K.transpose(convQues[batch_size,:,:]))\n",
    "thetaVQ = K.reshape(thetaVQ, (batch_size, 196, 15, 1))\n",
    "\n",
    "convQuesImage = Conv2D(1, kernel_size=(196, 1), activation='tanh')(thetaVQ)\n",
    "print(convQuesImage.get_shape())\n",
    "\n",
    "convImageQues = Conv2D(1, kernel_size=(1, 15), activation='tanh')(thetaVQ)\n",
    "print(convImageQues.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Question - Answer Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 15, 1)\n",
      "(1, 205, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "ans = Input(shape=(Na,512))\n",
    "convAns = Conv1D(512, kernel_size=1, activation=None)(ans)\n",
    "\n",
    "ques = Input(shape=(15,512))\n",
    "convQues = Conv1D(512, kernel_size=1, activation=None)(ques)\n",
    "batch_size = 1\n",
    "\n",
    "thetaAQ = K.dot(convAns[batch_size,:,:], K.transpose(convQues[batch_size,:,:]))\n",
    "thetaAQ = K.reshape(thetaAQ, (batch_size, Na, 15, 1))\n",
    "\n",
    "convAnsQues = Conv2D(1, kernel_size=(Na, 1), activation='tanh')(thetaAQ)\n",
    "print(convAnsQues.get_shape())\n",
    "\n",
    "convQuesAns = Conv2D(1, kernel_size=(1, 15), activation='tanh')(thetaAQ)\n",
    "print(convQuesAns.get_shape())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Answer - Image Potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 196, 1)\n",
      "(1, 205, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "ans = Input(shape=(Na,512))\n",
    "convAns = Conv1D(512, kernel_size=1, activation=None)(ans)\n",
    "\n",
    "img = Input(shape=(196,512))\n",
    "convImg = Conv1D(512, kernel_size=1, activation=None)(img)\n",
    "batch_size = 1\n",
    "\n",
    "thetaAV = K.dot(convAns[batch_size,:,:], K.transpose(convImg[batch_size,:,:]))\n",
    "thetaAV = K.reshape(thetaAV, (batch_size, Na, 196, 1))\n",
    "\n",
    "convAnsImg = Conv2D(1, kernel_size=(Na, 1), activation='tanh')(thetaAV)\n",
    "print(convAnsImg.get_shape())\n",
    "\n",
    "convImgAns = Conv2D(1, kernel_size=(1, 196), activation='tanh')(thetaAV)\n",
    "print(convImgAns.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15,)\n",
      "(196,)\n",
      "(15,)\n",
      "(205,)\n",
      "(196,)\n",
      "(205,)\n",
      "(205,)\n",
      "(196,)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "convQuesImage = K.reshape(convQuesImage, (15,))\n",
    "print(convQuesImage.shape)\n",
    "\n",
    "convImageQues = K.reshape(convImageQues, (196,))\n",
    "print(convImageQues.shape)\n",
    "\n",
    "convAnsQues = K.reshape(convAnsQues, (15,))\n",
    "print(convAnsQues.shape)\n",
    "\n",
    "convQuesAns = K.reshape(convQuesAns, (Na,))\n",
    "print(convQuesAns.shape)\n",
    "\n",
    "convAnsImg = K.reshape(convAnsImg, (196,))\n",
    "print(convAnsImg.shape)\n",
    "\n",
    "convImgAns = K.reshape(convImgAns, (Na,))\n",
    "print(convImgAns.shape)\n",
    "\n",
    "########### Reshaping Unary Potentials #######################\n",
    "Aoutput = K.reshape(Aoutput, (Na,))\n",
    "print(Aoutput.shape)\n",
    "\n",
    "Voutput = K.reshape(Voutput, (196,))\n",
    "print(Voutput.shape)\n",
    "\n",
    "Qoutput = K.reshape(Qoutput, (15,))\n",
    "print(Qoutput.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 205)\n",
      "(3, 15)\n",
      "(3, 196)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers import merge\n",
    "from keras.backend import stack\n",
    "\n",
    "conc_A = stack([convQuesAns, convImgAns, Aoutput], axis=0)\n",
    "conc_Q = stack([convAnsQues, convQuesImage, Qoutput], axis=0)\n",
    "conc_V = stack([convAnsImg, convImageQues, Voutput], axis=0)\n",
    "\n",
    "\n",
    "# conc_A = tf.concat(0,[convQuesAns, convImgAns, Aoutput])\n",
    "\n",
    "# conc_A = merge([convQuesAns, convImgAns, Aoutput], mode='concat', concat_axis=1)\n",
    "print(conc_A.shape)\n",
    "print(conc_Q.shape)\n",
    "print(conc_V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 3)\n",
      "(15, 3)\n",
      "(196, 3)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "conc_A = K.transpose(conc_A)\n",
    "conc_V = K.transpose(conc_V)\n",
    "conc_Q = K.transpose(conc_Q)\n",
    "\n",
    "print(conc_A.shape)\n",
    "print(conc_Q.shape)\n",
    "print(conc_V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(205, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 205, 3)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 205, 205)          820       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 205, 1)            206       \n",
      "=================================================================\n",
      "Total params: 1,026\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ANSWER \n",
    "print(conc_A.shape)\n",
    "test = Input(shape=(Na,3))\n",
    "denseA = Dense(Na, activation='relu')(test)\n",
    "outputA = Dense(1, activation='softmax')(denseA)\n",
    "dense_ans = Model(inputs=test, outputs=outputA)\n",
    "print(dense_ans.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 15, 3)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15, 80)            320       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15, 1)             81        \n",
      "=================================================================\n",
      "Total params: 401\n",
      "Trainable params: 401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# QUESTION\n",
    "print(conc_Q.shape)\n",
    "test = Input(shape=(15,3))\n",
    "denseA = Dense(Nq, activation='relu')(test)\n",
    "outputQ = Dense(1, activation='softmax')(denseA)\n",
    "dense_que = Model(inputs=test, outputs=outputQ)\n",
    "print(dense_que.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 3)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 196, 3)            0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 196, 205)          820       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 196, 1)            206       \n",
      "=================================================================\n",
      "Total params: 1,026\n",
      "Trainable params: 1,026\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Image\n",
    "print(conc_V.shape)\n",
    "test = Input(shape=(196,3))\n",
    "denseA = Dense(Na, activation='relu')(test)   # Img_count = hardcoded here\n",
    "outputV = Dense(1, activation='softmax')(denseA)\n",
    "dense_vis = Model(inputs=test, outputs=outputV)\n",
    "print(dense_vis.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATING ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans emb mat shape :  (205, 512)\n",
      "que emb mat shape :  (15, 512)\n",
      "img emb mat shape :  (196, 512)\n"
     ]
    }
   ],
   "source": [
    "# WHAT TO DO : multiply (op_dense) * (embedding)\n",
    "\n",
    "# FIRST STEP : GET EMBEDDING                                                        STATUS : \n",
    "#1 answe embedding => each of 205 is actually have d dimensions-- DICTIONARY!    == BUG! -> DONE!\n",
    "#2 quest embedding => using same PLACEHOLDER that was used earlier               == DONE\n",
    "#3 Image embedding => search for reshaped from getEm()                           == DONE\n",
    "\n",
    "\n",
    "#===  ANSWER EMBEDDING MATRIX ====\n",
    "ans_emb_mat = []\n",
    "for d in final_ans_embedding:\n",
    "    for k in d:\n",
    "        npArr = d[k]\n",
    "        ans_emb_mat.append (npArr)\n",
    "        \n",
    "ans_emb_mat = np.array(ans_emb_mat)\n",
    "print (\"ans emb mat shape : \", str (ans_emb_mat.shape))\n",
    "\n",
    "\n",
    "#===  QUESTION EMBEDDING MATRIX ====\n",
    "que_emb_mat = []\n",
    "for word in question.split(' '):\n",
    "    word = lib.tokenize(word)\n",
    "    if(len(word) > 1 and len(word[-1]) <= 1):\n",
    "        word = word[0]\n",
    "    word = ''.join(word)\n",
    "    word = word.lower()\n",
    "    if(word[-1] in ('?', ',')):\n",
    "        continue\n",
    "    que_emb_mat.append(question_embedding_map[word]) #Here\n",
    "    \n",
    "for i in range(15 - len(que_emb_mat)):\n",
    "    que_emb_mat.append(np.zeros(512))\n",
    "\n",
    "que_emb_mat = np.array(que_emb_mat)\n",
    "print (\"que emb mat shape : \", str (que_emb_mat.shape))\n",
    "    \n",
    "\n",
    "#===  IMAGE EMBEDDING MATRIX ====\n",
    "emb_name = 'COCO_train2014_000000000009.txt'\n",
    "threeD = all_img_emb[emb_name]\n",
    "img_emb_mat = []\n",
    "for i in range (14):\n",
    "    for j in range (14):\n",
    "        img_emb_mat.append (threeD[i][j])\n",
    "    \n",
    "img_emb_mat = np.array(img_emb_mat)\n",
    "\n",
    "print (\"img emb mat shape : \", str (img_emb_mat.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "(512,)\n"
     ]
    }
   ],
   "source": [
    "# converting np_to_tensor\n",
    "tf_VembMat = tf.convert_to_tensor(img_emb_mat, np.float32)\n",
    "tf_QembMat = tf.convert_to_tensor(que_emb_mat, np.float32)\n",
    "tf_AembMat = tf.convert_to_tensor(ans_emb_mat, np.float32)\n",
    "\n",
    "tf_outputV = tf.convert_to_tensor(outputV, np.float32)\n",
    "tf_outputQ = tf.convert_to_tensor(outputQ, np.float32)\n",
    "tf_outputA = tf.convert_to_tensor(outputA, np.float32)\n",
    "\n",
    "# FINDING ATTENTION : MAT_MUL (Pv * V), (Pa * A), (Pq * Q)\n",
    "attentionV = K.dot( K.transpose(tf_outputV[batch_size, :,:]) , tf_VembMat[:,:])\n",
    "attentionQ = K.dot( K.transpose(tf_outputQ[batch_size, :,:]) , tf_QembMat[:,:])\n",
    "attentionA = K.dot( K.transpose(tf_outputA[batch_size, :,:]) , tf_AembMat[:,:])\n",
    "\n",
    "print (type(attentionA))\n",
    "print (attentionA[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_2:0\", shape=(1, 512), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# using HADAMARD on all three.. element wise multiplication to get single [1xD]\n",
    "attQA = tf.multiply (attentionA, attentionQ)\n",
    "attQV = tf.multiply (attentionV, attentionQ)\n",
    "predictedTensor = tf.multiply (attQV, attQA)\n",
    "print (predictedTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cosine similarity of output of MCB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best matched ans # : 103\n"
     ]
    }
   ],
   "source": [
    "predictedArray = np.random.uniform(-1, 1, size=512)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cos_simi = cosine_similarity(predictedArray.reshape(1, -1), ans_emb_mat )\n",
    "ind = np.unravel_index(cos_simi.argmax(), cos_simi.shape)\n",
    "#print (\"max prob : \"+ str(cos_simi[ind]))\n",
    "print (\"best matched ans # : \" + str(ind[1]) )\n",
    "#print (ans_emb_mat[ind[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img id    : 262172\n",
      "Question  : Where are the two blue coolers?\n",
      "Answer    : left\n"
     ]
    }
   ],
   "source": [
    "print(\"img id    : \" + str (list_imageId_mapped_to_ques[ind[1]]))\n",
    "print (\"Question  : \" + str (list_questions[ind[1]]))\n",
    "print(\"Answer    : \" + str (list_ans_mapped_to_ques[ind[1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
